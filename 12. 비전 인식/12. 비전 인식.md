# 비전 인식 
비전 인식은 카메라를 통해 획득한 영상이나 이미지를 분석하여 사람처럼 사물을 이해하는 기술을 말합니다. 컴퓨터가 단순히 화면의 픽셀 정보를 받아들이는 수준을 넘어, 그 안에 있는 물체, 사람, 제스처, 움직임, 공간 정보 등을 스스로 인식하고 판단할 수 있도록 만드는 기술입니다.

현대 비전 인식 기술은 인공지능 기술과 결합해 빠르게 발전하고 있으며, 다양한 분야에서 핵심적인 역할을 수행합니다.

## 비전 인식이 왜 중요한가?
사람은 눈을 통해 수많은 정보를 직관적으로 이해합니다.
예를 들어 손을 들면 “들어 올렸다”고 판단하고, 물체가 다가오면 “가까워진다”고 느끼고,  사람의 표정을 보고 감정을 유추하고, 주변 환경을 보고 즉시 행동을 결정합니다.

컴퓨터도 이러한 능력을 갖추게 된다면, 다음과 같은 응용이 가능해집니다.

- 로봇이 사람의 움직임을 보고 따라 하기
- 스마트홈 장치가 사람의 손동작을 인식하여 조명·가전 제어
- 자율주행차가 주변 차량과 보행자를 인식
- 공장에서 제품의 이상 유무를 자동 검출
- 의료 영상에서 종양 검출
- 스포츠 분석, AR/VR 인터랙션, 게임 제어 등

즉, 비전 인식은 컴퓨터가 현실 세계와 상호작용하기 위한 핵심 기술입니다.

## MediaPipe
MediaPipe는 Google에서 개발한 멀티모달 ML 파이프라인 프레임워크로, 손/얼굴/포즈 등 다양한 비전 인식 기능을 제공합니다.

파이썬 환경에서는 별도의 딥러닝 모델을 직접 다루지 않아도, MediaPipe가 제공하는 사전 학습된 모델을 바로 불러와 간단한 코드로 손이나 얼굴을 인식할 수 있습니다.

MediaPipe 에 대한 더 자세한 설명은 아래 링크를 통해 확인해보시기 바랍니다. 

- [MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/guide?hl=ko)

MediaPipe 는 pip 명령을 통해 설치합니다. 
```sh
> pip install mediapipe
```

## OpenCV
OpenCV(Open Source Computer Vision Library)는 영상 처리와 컴퓨터 비전 기능을 제공하는 대표적인 라이브러리입니다.
파이썬과 함께 사용하면 웹캠에서 프레임을 받아와 화면에 출력하고, 영상 필터링, 특징 추출, 객체 검출 등 다양한 처리를 손쉽게 구현할 수 있습니다.

- [OpenCV](https://opencv.org/)

OpenCV 는 다음 명령을 통해 설치합니다. 
```sh
> pip install opencv-python
```

## 카메라 영상 출력하기 
처음으로 PC 에 연결된 카메라 영상을 출력해보겠습니다. 일반적인 데스크톱에서는 카메라가 없음으로 USB 형태의 웹캠등을 연결하여 실습을 진행합니다. 

PC 에 인식된 카메라의 영상을 실시간으로 읽어 출력하며, 키보드의 'q' 또는 'ESC' 를 누르면 종료됩니다. 

```python
import cv2

cam = cv2.VideoCapture(0)
while True:
    ret, frame = cam.read()
    cv2.imshow('frame', frame)
    key = cv2.waitKey(1) & 0xFF
    if key == 27 or key == ord('q'):
        break
```

VideoCapture() 를 통해 카메라 장치를 엽니다. 인자로 전달되는 숫자는 카메라의 번호로 PC 에 인식된 카메라를 구분하는 용도입니다. 1개만 연결된 상태라면 0, 2개가 연결되어 있다면 0 또는 1을 전달하여 카메라를 선택적으로 사용합니다. 

## 색공간 변환 
이미지의 색상을 변환할 때는 cvtColor() 메소드를 활용합니다. 인자로 변환할 이미지와 변환할 색상 공간에 대한 정보를 전달하면 색상 정보를 변경한 이미지를 반환해줍니다. cvtColor()에 색상 변환에 관한 인자를 몇 가지 소개하면 다음과 같습니다.

- cv2.COLOR_BGR2RGB, 
- cv2.COLOR_BGR2GRAY
- cv2.COLOR_RGB2HSV
- cv2.COLOR_RGB2YUV
-  etc..

```python
import cv2

cam = cv2.VideoCapture(0)

while True:
    ret, frame = cam.read()
    if not ret:
        break

    rgb  = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    cv2.imshow("Original (BGR)", frame)
    cv2.imshow("RGB", rgb)
    cv2.imshow("Grayscale", gray)

    if cv2.waitKey(1) & 0xFF in [27, ord('q')]:
        break
```

## ROI
영상 또는 이미지를 다루는 작업에서 전체의 화면 데이터를 활용하는 경우도 있지만 대부분 특정 구역의 픽셀들에서 특징을 추출하거나 색상을 변환하는 일련의 작업을 수행합니다. 이때 작업을 위해 원본 이미지에서 특정 영역을 잘라내는 것을 ROI(Region of Interest)를 지정한다고 합니다. ROI를 지정하는 방식은 데이터화 되어있는 이미지의 특정 구역의 데이터를 따로 저장하는 형태로 지정하는 것이 일반적입니다.

```python
import cv2

cam = cv2.VideoCapture(0)

while True:
    ret, frame = cam.read()
    if not ret:
        break
    h, w, _ = frame.shape
    cx, cy = w // 2, h // 2
    roi = frame[cy-100:cy+100, cx-100:cx+100]

    cv2.rectangle(frame, (cx-100, cy-100), (cx+100, cy+100), (0,255,0), 2)

    cv2.imshow("Camera", frame)
    cv2.imshow("ROI", roi)

    if cv2.waitKey(1) & 0xFF in [27, ord('q')]:
        break
```

## 손 인식 
다음 코드는 MediaPipe Hands를 이용해 손의 랜드마크를 인식하고, 각 손가락이 펴져 있는지/접혀 있는지를 간단한 규칙으로 판단해 제스처 이름을 출력하는 예제입니다.

```python
import cv2
import mediapipe as mp

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

FINGER_TIPS = [4, 8, 12, 16, 20]

def count_fingers(hand_landmarks, image_width, image_height, handedness):
    landmarks = hand_landmarks.landmark
    fingers = []
    thumb_tip = landmarks[4]
    thumb_ip  = landmarks[3]

    if handedness == "Right":
        fingers.append(thumb_tip.x < thumb_ip.x)
    else:
        fingers.append(thumb_tip.x > thumb_ip.x)

    for tip_idx, pip_idx in zip([8, 12, 16, 20], [6, 10, 14, 18]):
        tip = landmarks[tip_idx]
        pip = landmarks[pip_idx]
        fingers.append(tip.y < pip.y)

    return fingers.count(True)

def classify_gesture(finger_count):
    if finger_count == 0:
        return "FIST"
    elif finger_count == 1:
        return "ONE"
    elif finger_count == 2:
        return "TWO"
    elif finger_count == 3:
        return "THREE"
    elif finger_count == 4:
        return "FOUR"
    elif finger_count == 5:
        return "FIVE"
    else:
        return "UNKNOWN"

def main():
    cap = cv2.VideoCapture(0)

    cap.set(cv2.CAP_PROP_FRAME_WIDTH,  640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    with mp_hands.Hands(
        max_num_hands=2,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    ) as hands:

        while True:
            ret, frame = cap.read()
            if not ret:
                break
            else:
                frame = cv2.flip(frame,1)

            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image.flags.writeable = False
            results = hands.process(image)

            image.flags.writeable = True
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

            h, w, _ = image.shape

            if results.multi_hand_landmarks and results.multi_handedness:
                for hand_landmarks, handedness in zip(
                    results.multi_hand_landmarks,
                    results.multi_handedness
                ):
                    label = handedness.classification[0].label
                    finger_count = count_fingers(hand_landmarks, w, h, label)
                    gesture_name = classify_gesture(finger_count)
                    mp_drawing.draw_landmarks(image,hand_landmarks,mp_hands.HAND_CONNECTIONS)
                    wrist = hand_landmarks.landmark[0]
                    cx, cy = int(wrist.x * w), int(wrist.y * h)
                    cv2.putText(image,f"{label} : {gesture_name} ({finger_count})",(cx - 80, cy - 20),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0, 255, 0),2)
            cv2.imshow('Gesture Demo', image)
            key = cv2.waitKey(1) & 0xFF
            if key == 27 or key == ord('q'):
                break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```

## 손 인식을 통한 모터 제어 
이제 손 인식 결과를 실제 하드웨어 제어로 확장합니다. PC에서는 손의 방향(왼손/오른손/양손) 에 따라 서보 모터의 목표 각도를 계산하고, 그 값을 시리얼 통신으로 아두이노에 전달합니다. 아두이노는 전달받은 각도 값에 맞춰 서보 모터를 회전시키는 역할만 담당합니다.

구체적인 동작 시나리오는 다음과 같습니다.
- 시작 시 서보 각도는 90도(정중앙)
- 오른손만 인식되면 → 일정 주기마다 각도 증가(+1도씩)
- 왼손만 인식되면 → 일정 주기마다 각도 감소(-1도씩)
- 양손이 동시에 인식되고, 일정 시간(예: 5초) 이상 유지되면 → 다시 90도로 복귀
- 각도는 0도 미만, 180도 초과로 나가지 않도록 제한

### 하드웨어 연결 
| Pin | 연결 대상 | 설명 |
|:---:|:---:|:---|
| 5V | Red Cable | 전원 |
| GND | Brown Cable | 접지 |
| 13 | Yellow Cable | Servo Motor PWM |

### 아두이노 프로그램 
아두이노는 PC로부터 시리얼로 전송되는 각도 값(문자열) 을 읽어들이고, 해당 각도로 서보를 회전시키는 역할만 수행합니다.

```cpp
#include <Servo.h>

#define SERVO_PIN 13

Servo myServo;
int currentAngle = 90;
const int MIN_ANGLE = 0;
const int MAX_ANGLE = 180;

void setup() {
  Serial.begin(115200);
  myServo.attach(SERVO_PIN);

  myServo.write(currentAngle);
  Serial.println("READY");
}

void loop() {
  if (Serial.available() > 0) {
    String line = Serial.readStringUntil('\n');
    line.trim();

    if (line.length() > 0) {
      int angle = line.toInt();
      if (angle < MIN_ANGLE) angle = MIN_ANGLE;
      if (angle > MAX_ANGLE) angle = MAX_ANGLE;

      currentAngle = angle;
      myServo.write(currentAngle);
    }
  }
}
```

### PC 프로그램 
다음은 MediaPipe로 손을 인식하고, 인식 결과에 따라 서보 각도를 계산한 뒤 아두이노로 전송하는 전체 PC 측 프로그램입니다.

1. 아두이노 시리얼 포트 열기 및 "READY" 수신 대기
2. 카메라에서 프레임을 읽어 MediaPipe Hands로 손 인식
3. 엄지 방향으로 왼손/오른손 판정
4. 상태에 따라 서보 각도 점진적 변경
    - 오른손: 일정 주기마다 +STEP_ANGLE
    - 왼손: 일정 주기마다 -STEP_ANGLE
    - 양손(BOTH) 5초 이상: 중앙(90도)으로 복귀
5. 각도가 변경될 때마다 아두이노에 문자열로 전송

```python
import cv2
import mediapipe as mp
import serial
import time

ARDUINO_PORT = 'COM4'
BAUDRATE = 115200

ser = None

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

MIN_ANGLE = 0
MAX_ANGLE = 180
STEP_ANGLE = 1
STEP_INTERVAL = 0.05
BOTH_CENTER_TIME = 5.0

def open_arduino(port, baudrate, retries=5, retry_delay=0.5, wait_ready=True):
    global ser
    last_error = None
    for attempt in range(1, retries + 1):
        try:
            print(f"[INFO] Trying to open {port} (attempt {attempt}/{retries})")
            ser = serial.Serial(port, baudrate, timeout=1)

            if wait_ready:
                start = time.time()
                ready = False
                print("[INFO] Waiting for Arduino READY...")
                while time.time() - start < 3.0:
                    if ser.in_waiting:
                        line = ser.readline().decode(errors='ignore').strip()
                        print("[RX]", line)
                        if "READY" in line:
                            print("[INFO] Arduino says READY")
                            ready = True
                            break
                if not ready:
                    print("[WARN] READY not received, but continuing anyway.")
                ser.reset_input_buffer()
            return ser
        except serial.SerialException as e:
            print(f"[ERROR] Failed to open {port}: {e}")
            last_error = e
            time.sleep(retry_delay)
    raise RuntimeError(f"Could not open serial port {port}") from last_error

def get_hand_label_by_thumb(hand_landmarks):
    lm = hand_landmarks.landmark
    thumb_tip = lm[4].x
    thumb_mcp = lm[2].x

    if thumb_tip < thumb_mcp:
        return "Right"
    else:
        return "Left"

def send_angle(angle):
    global ser
    msg = f"{angle}\n"
    try:
        ser.write(msg.encode("utf-8"))
        print("[TX] Angle:", angle)
    except serial.SerialException as e:
        print("[ERROR] Serial write failed:", e)

def main():
    global ser
    ser = open_arduino(ARDUINO_PORT, BAUDRATE)

    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH,  640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    current_angle = 90
    send_angle(current_angle)

    last_state = None
    last_step_time = time.time()
    both_start_time = None

    with mp_hands.Hands(
        max_num_hands=2,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    ) as hands:

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame = cv2.flip(frame, 1)
            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image.flags.writeable = False
            results = hands.process(image)
            image.flags.writeable = True
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            has_left = False
            has_right = False
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    label = get_hand_label_by_thumb(hand_landmarks)
                    if label == "Left":
                        has_left = True
                    elif label == "Right":
                        has_right = True
                    mp_drawing.draw_landmarks(image,hand_landmarks,mp_hands.HAND_CONNECTIONS)
            if has_left and has_right:
                state = 'B'
                state_text = "BOTH"
            elif has_left:
                state = 'L'
                state_text = "LEFT"
            elif has_right:
                state = 'R'
                state_text = "RIGHT"
            else:
                state = 'N'
                state_text = "NONE"
            now = time.time()
            if state != last_state:
                print("[STATE]", state_text)
                last_state = state
                if state in ('L', 'R'):
                    last_step_time = now
                if state == 'B':
                    both_start_time = now
                else:
                    both_start_time = None

            if state == 'B':
                if both_start_time is not None and (now - both_start_time) >= BOTH_CENTER_TIME:
                    if current_angle != 90:
                        current_angle = 90
                        send_angle(current_angle)
                    both_start_time = None
            elif state == 'R':
                if (now - last_step_time) >= STEP_INTERVAL:
                    last_step_time = now
                    if current_angle < MAX_ANGLE:
                        current_angle += STEP_ANGLE
                        if current_angle > MAX_ANGLE:
                            current_angle = MAX_ANGLE
                        send_angle(current_angle)
            elif state == 'L':
                if (now - last_step_time) >= STEP_INTERVAL:
                    last_step_time = now
                    if current_angle > MIN_ANGLE:
                        current_angle -= STEP_ANGLE
                        if current_angle < MIN_ANGLE:
                            current_angle = MIN_ANGLE
                        send_angle(current_angle)
            cv2.putText(image, f"STATE: {state_text}", (10, 30),cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
            cv2.putText(image, f"ANGLE: {current_angle}", (10, 70),cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)

            cv2.imshow("Hand Control", image)
            key = cv2.waitKey(1) & 0xFF
            if key == 27 or key == ord('q'):
                break

    cap.release()
    cv2.destroyAllWindows()
    ser.close()

if __name__ == "__main__":
    main()
```